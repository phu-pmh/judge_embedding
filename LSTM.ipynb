{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0b950b9630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phu, Andrea and Watcher\n",
    "# 2018 Spring\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import FloatTensor, LongTensor\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import torch.utils.data as data_utils\n",
    "import psutil\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_current_memory_usage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    mem_use_in_GB = py.memory_info().rss/(2**30)\n",
    "    print(\"currently using\",mem_use_in_GB,\"GB memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# make sure you have the nltk resource downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"no nltk resource, downloading now\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data paths, so we can read data easily\n",
    "ruling_data_path = '/data/Dropbox/Projects/originalism/data/BloombergVOTELEVEL_Touse.dta'\n",
    "sentences_data_path = '/data/Dropbox/judge_embedding_data_sp18/sentences_data.csv'\n",
    "cite_graph_path = '/data/Dropbox/Data/corpora/chen-cases/cite-graph/graph.zip'\n",
    "judge_bio_data_path = '/data/Dropbox/Data/Judge-Bios/judgebios/JudgesBioReshaped_TOUSE.dta'\n",
    "topic_data_path = '/data/Dropbox/Projects/Ash_Chen/metadata/bb2topic.pkl'\n",
    "processed_data_path = '/data/Dropbox/judge_embedding_data_sp18'\n",
    "\n",
    "merged_sentence_data_path = '/data/Dropbox/judge_embedding_data_sp18/sentence_topic_judgeid.csv'\n",
    "\n",
    "meta_data_path = '/data/Dropbox/judge_embedding_data_sp18/circuit_metadata_excerpt.dta'\n",
    "table_of_cases_path = '/data/Dropbox/judge_embedding_data_sp18/tableofcases'\n",
    "\n",
    "judge_mapping_binary_filename = 'judgemap.pkl'\n",
    "\n",
    "# currently using 6B 300d glove, this one has 400K vocab\n",
    "glove_emb_path = '/data/Dropbox/judge_embedding_data_sp18/glove_files/glove.6B.300d.txt'\n",
    "glove_binary_filename = 'glove6B300d.pkl'\n",
    "\n",
    "opinion_sum_vector_final_merged_data_filename = '/data/Dropbox/judge_embedding_data_sp18/opinion_sum_vec_final.pkl'\n",
    "opinion_sum_vector_split_6_data_filename = '/data/Dropbox/judge_embedding_data_sp18/opinion_sum_vec_split6.pkl'\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_binary(processed_data_path,save_filename):\n",
    "    # after processed and saved glove binary, each time we use it we just load from the binary\n",
    "    # file name relative to processed data path\n",
    "    with open(os.path.join(processed_data_path,save_filename),\"rb\") as f:  \n",
    "        glove_emb, word2index, index2word = pickle.load(f) \n",
    "    return glove_emb, word2index, index2word\n",
    "glove_emb, word2index, index2word = load_glove_binary(processed_data_path,glove_binary_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def init_dict(word2index, index2word):\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            idx = len(self.word2index)\n",
    "            self.word2index[word] = idx\n",
    "            self.index2word[idx] = word\n",
    "    \n",
    "    \n",
    "    def add_sentences(self, sentences):\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                \n",
    "                self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "init_dict() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7d233d9e6f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#creating dictionary and init with word2index, index2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: init_dict() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#creating dictionary and init with word2index, index2word\n",
    "dictionary = Dictionary()\n",
    "dictionary.init_dict(word2index, index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.word2index[\"hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(opinion_sum_vector_final_merged_data_filename, 'rb') as pickle_file:\n",
    "    merged_sentence_data_df = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_sentence_data_df['opinion_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def train_val_test_split(data_df,number_judges,train_ratio=0.8,val_ratio=0.1,verbose=0):\n",
    "    # the input is the processed data\n",
    "    # first we sort it by judge embed index, this might make life easier for the split\n",
    "    # NOTE HERE THE DATA IS NOT SHUFFLED, SO LATER WE NEED TO SHUFFLE EACH DATASET\n",
    "    starttime= time.time()\n",
    "    \n",
    "    sorted_all_data = data_df.sort_values(by='judge_embed_index')\n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame() \n",
    "    for index in range(number_judges):\n",
    "        if verbose and index%100 == 0:\n",
    "            print(index,time.time()-starttime)\n",
    "        \n",
    "        cases_of_this_judge = sorted_all_data.loc[sorted_all_data['judge_embed_index'] == index]\n",
    "        shuffled_cases = shuffle(cases_of_this_judge) # we need it to be shuffled\n",
    "        \n",
    "        num_cases = shuffled_cases.shape[0]\n",
    "        n_of_train = int(num_cases*train_ratio)\n",
    "        n_of_val = int(num_cases*val_ratio)\n",
    "\n",
    "        train_df = train_df.append(shuffled_cases.iloc[:n_of_train,:])\n",
    "        val_df = val_df.append(shuffled_cases.iloc[n_of_train:n_of_train+n_of_val,:])\n",
    "        test_df = test_df.append(shuffled_cases.iloc[n_of_train+n_of_val:,:])\n",
    "    return train_df, val_df, test_df\n",
    "number_judges = 2099\n",
    "train_df, val_df, test_df = train_val_test_split(merged_sentence_data_df,number_judges,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape[0] + val_df.shape[0]+test_df.shape[0],merged_sentence_data_df.shape[0]) # should be the same\n",
    "show_current_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train size: \", train_df.shape[0])\n",
    "print(\"valid size: \", val_df.shape[0])\n",
    "print(\"test size: \", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_list = np.train_df[235440: 235445]['opinion_text'].values.tolist()\n",
    "# print(x_list)\n",
    "# x_list2 =  torch.LongTensor(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change words to word_ids\n",
    "def get_indices(sentences, dictionary):\n",
    "    sent_list = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "    \n",
    "        sent_indices = []\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            sent_indices.append(dictionary.word2index[word])\n",
    "        sent_list.append(sent_indices)\n",
    "    return sent_list\n",
    "\n",
    "def batchify(data, batch_size, use_cuda=False):\n",
    "    data_size = data.shape[0]\n",
    "    nbatch = (math.ceil(data_size/batch_size))\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    def list2batch(data_frame):\n",
    "        x_list = data_frame['opinion_text']\n",
    "        maxlen = max([len(x) for x in x_list])\n",
    "        input_tensor = torch.LongTensor(maxlen, b_size).fill_(0)\n",
    "        y = torch.LongTensor(data_frame['judge_embed_index'].as_matrix())\n",
    "        for idx, x in enumerate(x_list):\n",
    "            input_tensor[:len(x), idx] = torch.LongTensor(x)\n",
    "            \n",
    "        if use_cuda:\n",
    "            input_tensor = input.cuda()\n",
    "            y = y.cuda()\n",
    "        return input_tensor, y\n",
    "\n",
    "    data_batched = []\n",
    "    for i in range(nbatch):\n",
    "        batch = data[i * bsz: (i + 1) * bsz]\n",
    "        opinion_texts_tensor, judge_emb_idx = list2batch(batch)\n",
    "        data_batched.append((opinion_texts_tensor, judge_emb_idx))\n",
    "\n",
    "    return data_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "X_train, y_train = batchify(train_df, BATCH_SIZE)\n",
    "X_val, y_val = batchify(val_df, BATCH_SIZE)\n",
    "X_test, y_test = batchify(test_df, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, word_emb_dim, encoder_dim, vocab_size, num_layers=1, dropout=0.3):\n",
    "        super(BLSTMEncoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.enc_lstm_dim = encoder_dim\n",
    "        self.pool_type = 'max'\n",
    "        self.dpout_model = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
    "                                bidirectional=True, batch_first = True,dropout=self.dpout_model)\n",
    "        #self.init_embedding()\n",
    "\n",
    "\n",
    "    def forward(self, x, evaluation_mode = False):\n",
    "        # Set initial states\n",
    "        memory_states = (Variable(torch.zeros(self.num_layers*2, len(x), self.enc_lstm_dim), requires_grad=evaluation_mode),\n",
    "              Variable(torch.zeros(self.num_layers*2, len(x), self.enc_lstm_dim), requires_grad=evaluation_mode))\n",
    "        \n",
    "        emb = self.embed(Variable(x, requires_grad=evaluation_mode)) #get word embedding\n",
    "        emb = self.drop(emb)\n",
    "        # Forward propagate LSTM\n",
    "        out, hidden = self.enc_lstm(emb, memory_states)\n",
    "        \n",
    "        # max pooling\n",
    "        out = torch.max(out, 0)[0]\n",
    "        return out\n",
    "    \n",
    "    def init_embedding(self):\n",
    "        initrange = 0.1\n",
    "        self.enc_lstm.weight.data.uniform_(-initrange, initrange)\n",
    "        self.enc_lstm.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "LSTM_HIDDEN_UNITS = 300\n",
    "VOCAB_SIZE = len(dictionary.word2index)\n",
    "opinion_encoder = BLSTMEncoder(BATCH_SIZE, EMBED_DIM, LSTM_HIDDEN_UNITS, VOCAB_SIZE)\n",
    "sent_output = opinion_encoder(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
